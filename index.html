<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VIEXAM123"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-VIEXAM123');
  </script>

  <meta charset="utf-8">
  <meta name="description" content="ViExam: The first comprehensive evaluation of Vision Language Models on Vietnamese multimodal exam questions">
  <meta name="keywords" content="VLM, Vision Language Models, Vietnamese, Multimodal, Education, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- SỬA 1: THAY ĐỔI TIÊU ĐỀ TAB -->
  <title>VMMU</title>
  
  <!-- SỬA 2: THAY ĐỔI ICON TAB (FAVICON) -->
  <link rel="icon" href="images/conical.png" type="image/png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="style.css">
</head>
<body>

<!-- =================== HERO SECTION (AFFILIATION CLEANED) =================== -->
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="has-text-centered">
        
        <!-- Title with inline icon -->
        <h1 class="title">
          <img src="images/conical.png" alt="VMMU Project Icon" class="title-icon">
          VMMU: A Vietnamese Multitask Multimodal Understanding and Reasoning Benchmark
        </h1>
        
        <!-- Authors (đã xóa số 1) -->
        <div class="is-size-4 publication-authors">
          <span class="author-block">
            <a href="https://www.linkedin.com/in/dang-thi-tuong-vy-00a357278/" target="_blank">Vy Tuong Dang</a><sup>*1</sup>,
          </span>
          <span class="author-block">
            <a href="https://anvo25.github.io/" target="_blank">An Vo</a><sup>*1,2</sup>,
          </span>
          <span class="author-block">
            <a href="https://villacu.github.io/" target="_blank">Emilio Villa-Cueva</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a href="https://www.linkedin.com/in/quang-tau-a708b4238/" target="_blank">Quang Tau</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a href="https://www.resl.kaist.ac.kr/members/master-student#h.fiaa4al7sz8u" target="_blank">Duc Dm</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a href="https://mbzuai.ac.ae/study/faculty/thamar-solorio/" target="_blank">Thamar Solorio</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a href="https://www.resl.kaist.ac.kr/members/director" target="_blank">Daeyoung Kim</a><sup>1</sup>
          </span>
        </div>
        <div class="is-size-5">
          <sup>*</sup>Equal contribution
        </div>

        <!-- Affiliation (đã xóa số 1) -->
        <div class="is-size-5 publication-authors has-text-centered">
          <span class="author-block"><sup>1</sup>KAIST</span>
          <span class="author-block"><sup>2</sup>MBZUAI</span>
        </div>
        
        <!-- Buttons -->
        <div class="mt-4 has-text-centered">
          <div class="buttons is-centered">
            <a href="https://arxiv.org/abs/2508.13680" class="button is-rounded is-dark" target="_blank" rel="noopener noreferrer">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>Paper</span>
            </a>
            <a href="https://github.com/TuongVy20522176/ViExam" class="button is-rounded is-dark" target="_blank" rel="noopener noreferrer">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
            <a href="https://huggingface.co/datasets/anvo25/viexam" class="button is-rounded is-dark" target="_blank" rel="noopener noreferrer">
              <span class="icon"><i class="fas fa-database"></i></span>
              <span>Dataset</span>
            </a>
            <br>
            <a href="#example-gallery-section" class="button is-rounded is-examples">
                <span class="icon"><i class="fas fa-graduation-cap"></i></span>
                <span>Explore Exam Questions</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- =================== FINAL POLISHED TLDR (Centered Header) =================== -->
<section class="section tldr-section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-two-thirds">
        <div class="tldr-card-single">
          <!-- Thêm class has-text-centered để căn giữa badge -->
          <div class="tldr-header has-text-centered">
            <span class="tldr-badge">
              <!-- Đổi icon thành thumbtack -->
              <i class="fas fa-thumbtack"></i> TL;DR
            </span>
          </div>
          <div class="tldr-content">
            <p>
              We introduce <strong>VMMU</strong>, a Vietnamese Multitask Multimodal Understanding and Reasoning Benchmark with <strong>2.5k multimodal questions</strong> across <strong>7 tasks</strong>. Despite strong Vietnamese OCR performance, proprietary models achieve only <strong>66% mean accuracy</strong>. Analysis shows the primary sources of failure are <strong>multimodal grounding and reasoning</strong> over text and visual evidence, rather than OCR.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- =================== FINAL TEASER SECTION (WIDER TEXT FRAME) =================== -->
<section class="section">
  <div class="container">

    <!-- KHỐI VĂN BẢN -->
    <div class="columns is-centered">
      <!-- THAY ĐỔI CHÍNH Ở ĐÂY: is-four-fifths -> is-full-width -->
      <div class="column is-full-width">
        <div class="abstract-card">
          <div class="abstract-header-clean">
            <h2 class="abstract-title-clean">
              <i class="fas fa-microscope"></i>
              Overview
            </h2>
          </div>
          <div class="abstract-text-clean">
            <p>
              <span class="term-highlight">Vision Language Models (VLMs)</span> have made rapid progress on multimodal benchmarks, demonstrating strong performance on complex visual and textual reasoning tasks in English. However, it remains unclear how well these models perform in <span class="term-highlight">low-resource languages</span>, especially when language understanding, visual grounding, and reasoning are all required to complete a task.
            </p>
            <p>
              We introduce <span class="term-highlight">VMMU</span>, a Vietnamese Multitask Multimodal Understanding and Reasoning Benchmark designed to evaluate how vision language models interpret and reason over visual and textual information <span class="term-highlight">beyond English</span>. VMMU consists of <strong>2.5k multimodal questions</strong> across <strong>7 tasks</strong>, covering a diverse range of problem contexts, including <strong>STEM problem solving, data interpretation, rule-governed visual reasoning, and abstract visual reasoning</strong>.
            </p>
            <p>
              All questions require <span class="term-highlight">genuine multimodal integration</span>, rather than reliance on text-only cues or OCR-based shortcuts. We evaluate a diverse set of state-of-the-art (SOTA) proprietary VLMs on VMMU and find that despite strong Vietnamese OCR performance, proprietary models achieve only <strong>66% mean accuracy</strong>. Further analysis shows that the primary sources of failure are not OCR, but instead <span class="term-highlight">multimodal grounding and reasoning</span> over text and visual evidence.
            </p>
          </div>
        </div>
      </div>
    </div>

    <!-- KHỐI HÌNH ẢNH (Không đổi) -->
    <div class="full-width-image-container">
      <img src="images/question_image.jpg" 
           alt="Example of a Vietnamese multimodal exam question from our ViExam benchmark" 
           class="teaser-image-clean">
      <p class="image-caption-clean">
        Example of a Vietnamese multimodal exam question from ViExam, combining text and visual elements.
      </p>
    </div>
    
  </div>
</section>


  <!-- PHẦN 7 KEY FINDINGS - CÓ NỀN XANH NHẠT -->
  <section class="section section-key-findings">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="key-insight">
            <h3><i class="fas fa-microscope"></i> 5 Key Research Findings</h3>
            <div class="results-grid">
              <!-- Finding 1 -->
              <div class="model-result">
                <h4><a href="#performance-table" class="finding-link"><i class="fas fa-chart-line"></i> Proprietary VLMs Score Relatively Low</a></h4>
                <p>Non-thinking proprietary VLMs achieve only 50-71% accuracy on VMMU, while thinking VLMs perform significantly better at 73-86%. This indicates substantial room for improvement in Vietnamese multimodal understanding.</p>
              </div>
              <!-- Finding 2 -->
              <div class="model-result">
                <h4><a href="#ocr-table" class="finding-link"><i class="fas fa-eye"></i> OCR Is Not the Bottleneck</a></h4>
                <p>Across 5 proprietary SOTA VLMs, Vietnamese embedded-text transcription is consistently strong (mean BLEU 89.01%, F1 94.30%, CER 6.59%, WER 9.33%). Most remaining errors are attributable to multimodal grounding and downstream reasoning, not OCR failures.</p>
              </div>
              <!-- Finding 3 -->
              <div class="model-result">
                <h4><a href="#performance-table" class="finding-link"><i class="fas fa-image"></i> Disentangling Text from Visual Evidence Improves Performance</a></h4>
                <p>When the question and options are given as text instead of being rendered within the image (Split-MM), accuracy increases for every model (+6 percentage points on average), showing that separating text from visual evidence reduces interference and improves multimodal reasoning.</p>
              </div>
              <!-- Finding 4 -->
              <div class="model-result">
                <h4><a href="#crosslingual-table" class="finding-link"><i class="fas fa-language"></i> English Translation Does Not Help</a></h4>
                <p>Translating the Vietnamese text into English consistently reduces accuracy for all models (-2 percentage points on average). This suggests that Vietnamese language understanding is not the primary bottleneck, and translation introduces mismatches that degrade grounded multimodal reasoning.</p>
              </div>
              <!-- Finding 5 -->
              <div class="model-result">
                <h4><a href="#performance-table" class="finding-link"><i class="fas fa-database"></i> Removing Visual Evidence Reveals Possible Reliance on Priors</a></h4>
                <p>When visual evidence is removed, accuracy drops significantly (-21.27 points); yet VLMs still perform above random chance (25.92%), suggesting substantial reliance on text-only priors and exam heuristics rather than genuine visual grounding.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- PHẦN ADDITIONAL FINDINGS - NỀN HỒNG NHẠT -->
  <section class="section section-additional-findings">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="additional-insight">
            <h3><i class="fas fa-lightbulb"></i> Additional Findings</h3>
            <div class="results-grid">
              <!-- Additional Finding A.1 -->
              <div class="model-result">
                <h4><i class="fas fa-image"></i> A.1 Image-Required Questions Are Harder</h4>
                <p>Questions that truly require image understanding are significantly more challenging than text-sufficient questions, demonstrating the benchmark's focus on visual reasoning.</p>
              </div>
              <!-- Additional Finding A.2 -->
              <div class="model-result">
                <h4><i class="fas fa-random"></i> A.2 Answer Shuffling Has Minor Effect</h4>
                <p>Shuffling answer options has a minor impact on model performance, indicating models are not heavily reliant on answer position biases.</p>
              </div>
              <!-- Additional Finding A.3 -->
              <div class="model-result">
                <h4><i class="fas fa-sync-alt"></i> A.3 Multiple Runs Show Consistent Rankings</h4>
                <p>Running evaluations multiple times does not change model rankings or main conclusions, confirming the robustness of the benchmark results.</p>
              </div>
              <!-- Additional Finding A.4 -->
              <div class="model-result">
                <h4><i class="fas fa-exclamation-triangle"></i> A.4 Non-Thinking VLMs Are Overconfident</h4>
                <p>Vision-language models without reasoning capabilities tend to be overconfident in their incorrect predictions.</p>
              </div>
              <!-- Additional Finding A.5 -->
              <div class="model-result">
                <h4><i class="fas fa-check-circle"></i> A.5 SOTA VLMs Follow Answer Format Reliably</h4>
                <p>State-of-the-art vision-language models consistently follow the required answer format, demonstrating strong instruction-following capabilities.</p>
              </div>
              <!-- Additional Finding A.6 -->
              <div class="model-result">
                <h4><i class="fas fa-unlock-alt"></i> A.6 Open-Source VLMs Significantly Underperform</h4>
                <p>Open-source vision-language models show substantial performance gaps compared to proprietary models on VMMU benchmark tasks.</p>
              </div>
              <!-- Additional Finding A.7 -->
              <div class="model-result">
                <h4><i class="fas fa-font"></i> A.7 Weak OCR Drives Open-Source Failures</h4>
                <p>The primary weakness of open-source VLMs is their inferior embedded-text OCR capabilities, which significantly impacts overall performance.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


<section class="section">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">

          <div class="content-section">
            <h2><i class="fas fa-cogs icon-large"></i> How We Built ViExam: The Data Curation Pipeline</h2>
            <p>
              To ensure the quality and validity of our benchmark, we developed a systematic 3-stage pipeline to collect, filter, and classify exam questions. This process was crucial for building a dataset that genuinely tests multimodal reasoning, moving beyond simple text-only questions found in previous benchmarks.
            </p>
  
            <div class="research-figure" style="max-width: 100%;">
              <img src="images/data_generator.jpg" alt="VMMU's data curation pipeline">
              <div class="figure-caption">
                Our three-stage data curation process: (1) Sourcing raw exam PDFs and converting them to images, (2) an automated pipeline to detect and classify questions, and (3) a final manual verification loop by native Vietnamese speakers.
              </div>
            </div>
  
            <h4>Our 3-Stage Process Explained:</h4>
            <p>
              <strong>Stage 1: Data Sourcing & Conversion.</strong> We began by automatically crawling thousands of exam papers from popular Vietnamese educational websites. Each paper was then converted into high-resolution images, preparing them for analysis.
            </p>
            <p>
              <strong>Stage 2: Automated Classification.</strong> Next, our custom pipeline used OCR to identify individual questions on each page. A specialized analyzer then scanned the content of each question, automatically distinguishing multimodal (image-based) questions from text-only ones.
            </p>
            <p>
              <strong>Stage 3: Human Verification.</strong> Finally, to ensure maximum accuracy, every single question was manually reviewed by a team of native Vietnamese speakers. They used a custom-built tool to correct any classification errors and validate the final dataset.
            </p>
          </div>
  
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container">
    <div class="has-text-centered">
      <h2 class="title"><i class="fas fa-book-open" style="margin-right: 0.8rem;"></i>Dataset at a Glance</h2>
      <p class="subtitle">Our ViExam benchmark features 2,548 questions across 7 diverse domains.</p>
    </div>

    <div class="domain-grid">
      <!-- 1. Mathematics -->
      <div class="domain-card">
        <img src="images/math_icon.png" alt="Mathematics Icon" class="domain-icon">
        <h3 class="domain-name">Mathematics</h3>
        <p class="domain-count">456 Questions</p>
      </div>
      <!-- 2. Physics -->
      <div class="domain-card">
        <img src="images/physics_icon.png" alt="Physics Icon" class="domain-icon">
        <h3 class="domain-name">Physics</h3>
        <p class="domain-count">361 Questions</p>
      </div>
      <!-- 3. Chemistry -->
      <div class="domain-card">
        <img src="images/chemistry_icon.png" alt="Chemistry Icon" class="domain-icon">
        <h3 class="domain-name">Chemistry</h3>
        <p class="domain-count">302 Questions</p>
      </div>
      <!-- 4. Biology -->
      <div class="domain-card">
        <img src="images/biology_icon.png" alt="Biology Icon" class="domain-icon">
        <h3 class="domain-name">Biology</h3>
        <p class="domain-count">341 Questions</p>
      </div>
      <!-- 5. Geography -->
      <div class="domain-card">
        <img src="images/geography_icon.png" alt="Geography Icon" class="domain-icon">
        <h3 class="domain-name">Geography</h3>
        <p class="domain-count">481 Questions</p>
      </div>
      <!-- 6. Driving Test -->
      <div class="domain-card">
        <img src="images/driving_icon.png" alt="Driving Test Icon" class="domain-icon">
        <h3 class="domain-name">Driving Test</h3>
        <p class="domain-count">367 Questions</p>
      </div>
      <!-- 7. IQ Test -->
      <div class="domain-card">
        <img src="images/iq_icon.png" alt="IQ Test Icon" class="domain-icon">
        <h3 class="domain-name">IQ Test</h3>
        <p class="domain-count">240 Questions</p>
      </div>
    </div>
  </div>
</section>


<!-- PHẦN SAMPLE QUESTIONS - PHIÊN BẢN SLIDER -->
<section class="section" id="example-gallery-section">
  <div class="container">
      <div class="gallery-section-slider">
          <div class="gallery-header-slider">
              <div class="header-title-slider">
                  <h2><i class="fas fa-graduation-cap"></i> Sample Exam Questions</h2>
                  <p>Explore authentic Vietnamese multimodal exam questions from our benchmark.</p>
              </div>
              <div class="gallery-navigation">
                  <button class="gallery-nav prev" id="prev-slide-btn">&lt;</button>
                  <span class="slide-counter" id="slide-counter">1 / 3</span>
                  <button class="gallery-nav next" id="next-slide-btn">&gt;</button>
              </div>
          </div>
          
          <div class="gallery-viewport-slider">
            <div class="gallery-track-slider" id="gallery-track">
              <!-- Slides will be injected here by JavaScript -->
            </div>
          </div>
          
          <div class="gallery-filters" id="gallery-filters">
            <div class="filters-loading">Loading domains...</div>
          </div>
      </div>
  </div>
</section>
<!-- 
PHẦN NHÚNG TRANG WEB CHECK.HTML -->
<!-- <section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content-section">
          
          <div class="has-text-centered">
            <h2 class="title"><i class="fas fa-tasks" style="margin-right: 0.8rem;"></i>Our Annotation Interface</h2>
            <p class="subtitle">
              This is the live interface our team used for the manual verification process (Stage 3), ensuring the quality of every question in the ViExam benchmark.
            </p>
          </div>

          <div class="iframe-container">
            <iframe 
                src="check.html" 
                width="100%" 
                height="800px" 
                style="border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);"
                title="ViExam Question Review System Interface">
                Your browser does not support iframes.
            </iframe>
          </div>

        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content-section">
          <h2 id="performance-table"><i class="fas fa-chart-bar icon-large"></i>Overall Performance</h2>
          
          <div class="research-table">
            <div class="comparison-table">
              <table>
                <caption>
                  SOTA proprietary VLMs (68.70%) significantly outperform human average (66.54%) and open-source VLMs (35.53%). Human performance is based on official Vietnamese national high school graduation exam results (VnExpress, 2024).
                </caption>
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>a. Math</th>
                    <th>b. Physics</th>
                    <th>c. Chemistry</th>
                    <th>d. Biology</th>
                    <th>e. Geography</th>
                    <th>f. Driving</th>
                    <th>g. IQ</th>
                    <th>Mean</th>
                  </tr>
                </thead>
                <tbody>
                  <!-- Human & Random Baseline -->
                  <tr style="background-color: #f0fdf4;">
                    <td><strong>Human (Average)</strong></td>
                    <td>64.50</td>
                    <td>66.70</td>
                    <td>66.80</td>
                    <td>62.80</td>
                    <td>71.90</td>
                    <td>–</td>
                    <td>–</td>
                    <td>66.54</td>
                  </tr>
                  <tr style="background-color: #dcfce7;">
                    <td><strong>Human (Best)</strong></td>
                    <td>98.00</td>
                    <td>100.0</td>
                    <td>100.0</td>
                    <td>100.0</td>
                    <td>100.0</td>
                    <td>–</td>
                    <td>–</td>
                    <td>99.60</td>
                  </tr>
                  <tr style="background-color: #f8fafc;">
                    <td>Random baseline</td>
                    <td>25.00</td>
                    <td>24.66</td>
                    <td>24.38</td>
                    <td>24.67</td>
                    <td>25.00</td>
                    <td>33.24</td>
                    <td>24.46</td>
                    <td>25.92</td>
                  </tr>

                  <!-- Open-source VLMs -->
                  <tr>
                    <td colspan="9" class="table-subtitle" style="background-color: #f1f5f9; font-weight: bold; text-align: center;">Open-source VLMs</td>
                  </tr>
                  <tr>
                    <td>Aya-Vision-8B</td>
                    <td>10.75</td>
                    <td>8.03</td>
                    <td>3.64</td>
                    <td>3.23</td>
                    <td>2.49</td>
                    <td>26.98</td>
                    <td>12.50</td>
                    <td>9.66</td>
                  </tr>
                  <tr>
                    <td>Aya-Vision-32B</td>
                    <td>12.94</td>
                    <td>14.40</td>
                    <td>16.89</td>
                    <td>17.30</td>
                    <td>21.41</td>
                    <td>32.97</td>
                    <td>24.58</td>
                    <td>20.07</td>
                  </tr>
                  <tr>
                    <td>Gemma-3-4B</td>
                    <td>26.32</td>
                    <td>17.45</td>
                    <td>22.19</td>
                    <td>22.29</td>
                    <td>27.65</td>
                    <td>40.60</td>
                    <td>21.67</td>
                    <td>25.45</td>
                  </tr>
                  <tr>
                    <td>Gemma-3-27B</td>
                    <td>47.37</td>
                    <td>30.47</td>
                    <td>38.74</td>
                    <td>30.79</td>
                    <td>47.40</td>
                    <td>43.87</td>
                    <td>35.00</td>
                    <td>39.09</td>
                  </tr>
                  <tr>
                    <td>Qwen-2.5-VL-32B</td>
                    <td><strong>61.18</strong></td>
                    <td>49.17</td>
                    <td>44.70</td>
                    <td><strong>42.82</strong></td>
                    <td><strong>71.31</strong></td>
                    <td><strong>54.50</strong></td>
                    <td>41.25</td>
                    <td><strong>52.13</strong></td>
                  </tr>
                  <tr>
                    <td>Qwen-2.5-VL-72B</td>
                    <td>58.33</td>
                    <td><strong>49.86</strong></td>
                    <td><strong>48.34</strong></td>
                    <td>40.47</td>
                    <td>64.66</td>
                    <td>53.95</td>
                    <td><strong>46.67</strong></td>
                    <td>51.75</td>
                  </tr>
                  <tr>
                    <td>Llama-4-Scout</td>
                    <td>59.65</td>
                    <td>44.88</td>
                    <td>45.03</td>
                    <td>38.71</td>
                    <td>69.02</td>
                    <td>49.05</td>
                    <td>37.08</td>
                    <td>49.06</td>
                  </tr>
                  <tr>
                    <td>Llama-4-Maverick</td>
                    <td>42.54</td>
                    <td>29.36</td>
                    <td>32.78</td>
                    <td>27.57</td>
                    <td>53.22</td>
                    <td>53.95</td>
                    <td>29.17</td>
                    <td>38.37</td>
                  </tr>
                  <tr>
                    <td>Mistral-Small-3.2-24B</td>
                    <td>34.65</td>
                    <td>30.28</td>
                    <td>28.48</td>
                    <td>28.15</td>
                    <td>30.98</td>
                    <td>39.78</td>
                    <td>30.83</td>
                    <td>31.88</td>
                  </tr>
                  <tr>
                    <td>Mistral-Medium-3</td>
                    <td>44.74</td>
                    <td>36.57</td>
                    <td>37.75</td>
                    <td>29.33</td>
                    <td>36.59</td>
                    <td>45.78</td>
                    <td>33.75</td>
                    <td>37.78</td>
                  </tr>
                  <tr style="background-color: #f8fafc; font-weight: bold;">
                    <td>Mean</td>
                    <td>39.85</td>
                    <td>31.05</td>
                    <td>31.85</td>
                    <td>28.06</td>
                    <td>42.47</td>
                    <td>44.14</td>
                    <td>31.25</td>
                    <td>35.53</td>
                  </tr>

                  <!-- SOTA Proprietary VLMs -->
                  <tr>
                    <td colspan="9" class="table-subtitle" style="background-color: #f1f5f9; font-weight: bold; text-align: center;">SOTA proprietary VLMs</td>
                  </tr>
                  <tr>
                    <td>Gemini-2.5-Flash</td>
                    <td>82.46</td>
                    <td>67.04</td>
                    <td>78.15</td>
                    <td>63.05</td>
                    <td>85.24</td>
                    <td>71.39</td>
                    <td>52.08</td>
                    <td>71.34</td>
                  </tr>
                  <tr>
                    <td>Sonnet-4.0</td>
                    <td>64.25</td>
                    <td>41.00</td>
                    <td>53.31</td>
                    <td>44.87</td>
                    <td>48.44</td>
                    <td>58.04</td>
                    <td>44.17</td>
                    <td>50.58</td>
                  </tr>
                  <tr>
                    <td>GPT-4.1</td>
                    <td>46.27</td>
                    <td>43.21</td>
                    <td>44.37</td>
                    <td>44.87</td>
                    <td>69.85</td>
                    <td>66.21</td>
                    <td>46.25</td>
                    <td>51.58</td>
                  </tr>
                  <tr>
                    <td>o3</td>
                    <td>84.87</td>
                    <td>68.98</td>
                    <td>82.78</td>
                    <td>67.16</td>
                    <td>88.98</td>
                    <td>74.66</td>
                    <td>50.42</td>
                    <td>73.98</td>
                  </tr>
                  <tr>
                    <td>Gemini-3.0-Pro</td>
                    <td><strong>92.54</strong></td>
                    <td><strong>81.16</strong></td>
                    <td><strong>91.72</strong></td>
                    <td><strong>80.94</strong></td>
                    <td><strong>94.59</strong></td>
                    <td><strong>91.28</strong></td>
                    <td><strong>72.08</strong></td>
                    <td><strong>86.33</strong></td>
                  </tr>
                  <tr>
                    <td>GPT-5.0</td>
                    <td>91.23</td>
                    <td>76.45</td>
                    <td>85.10</td>
                    <td>72.73</td>
                    <td>89.40</td>
                    <td>75.20</td>
                    <td>58.75</td>
                    <td>78.41</td>
                  </tr>
                  <tr style="background-color: #f8fafc; font-weight: bold;">
                    <td>Mean</td>
                    <td>76.94</td>
                    <td>62.97</td>
                    <td>72.57</td>
                    <td>62.27</td>
                    <td>79.42</td>
                    <td>72.80</td>
                    <td>53.96</td>
                    <td>68.70</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- PHẦN KẾT LUẬN MỚI THEO SƠ ĐỒ -->
<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content-section">
          <h2><i class="fas fa-flag-checkered icon-large"></i>Conclusion</h2>

          <!-- Hàng trên: Primary Contribution -->
          <div class="columns is-centered">
            <div class="column">
              <div class="conclusion-box">
                <h3 class="title is-4">Primary Contribution</h3>
                <p>This study documents systematic challenges in VLMs' Vietnamese multimodal reasoning through comprehensive evaluation across 7 academic domains, demonstrating that failures stem from multimodal integration rather than basic text recognition limitations.</p>
              </div>
            </div>
          </div>

          <!-- Hàng dưới: Key Results & Core Finding -->
          <div class="columns is-centered">
            <!-- Cột Key Results -->
            <div class="column is-half">
              <div class="conclusion-box">
                <h3 class="title is-5">Key Results</h3>
                <p>SOTA VLMs achieved only 57.74% accuracy on Vietnamese multimodal exam questions while open-source models averaged 27.70%, both underperforming average human test-takers (66.54%).</p>
              </div>
            </div>
            <!-- Cột Core Finding -->
            <div class="column is-half">
              <div class="conclusion-box">
                <h3 class="title is-5">Core Finding</h3>
                <p>VLMs consistently struggle with Vietnamese multimodal content integration, with thinking models like o3 substantially outperforming non-thinking models (74.07% vs 48.28-59.87%), suggesting that future work on tool-augmented reasoning is needed to close this gap and achieve better performance on complex reasoning tasks</p>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8 has-text-centered">
        <p class="footer-text">
          This website is adapted from <a href="https://nerfies.github.io">Nerfies</a>.
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="script.js"></script>
</body>
</html>